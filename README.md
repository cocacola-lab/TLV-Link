# Touch100k: A Large-Scale Touch-Language-Vision Dataset for Touch-Centric Multimodal Representation

<p align="center">
  <img src="assets/data_present.png">
</p>

## ❤️ Acknowledgement
* [LanguageBind](https://github.com/PKU-YuanGroup/LanguageBind): An open source, language-based multimodal pre-training framework. Thanks for their wonderful work.
* [OpenCLIP](https://github.com/mlfoundations/open_clip): An amazing open-sourced backbone.
