# Touch100k

ğŸš€ğŸš€ğŸš€ The implementation of **Touch100k: A Large-Scale Touch-Language-Vision Dataset for Touch-Centric Multimodal Representation**.


<p align="center">
  <img src="assets/data_present.svg" width="600" style="margin-bottom: 0.2;"/>
</p>

## â¤ï¸ Acknowledgement
* [LanguageBind](https://github.com/PKU-YuanGroup/LanguageBind): An open source, language-based multimodal pre-training framework. Thanks for their wonderful work.
* [OpenCLIP](https://github.com/mlfoundations/open_clip): An amazing open-sourced backbone.
